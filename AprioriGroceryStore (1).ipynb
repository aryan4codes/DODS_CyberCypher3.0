{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANBUip3EnhkF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "\n",
        "# ----------------------------\n",
        "# INSTALL\n",
        "# !pip install mlxtend\n",
        "\n",
        "# ----------------------------\n",
        "# TRANSACTION ENCODER\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "# ----------------------------\n",
        "# APRIORI FUNCTION\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# ----------------------------\n",
        "# ITERTOOLS\n",
        "import itertools\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIGURATION\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/CyberCypher3.0_Project/grocery_database.xlsx\",names=['products'],header=None)\n",
        "df\n",
        "data = list(df[\"products\"].apply(lambda x:x.split(',')))\n",
        "\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "te = TransactionEncoder()\n",
        "te_data = te.fit(data).transform(data)\n",
        "df = pd.DataFrame(te_data,columns=te.columns_).astype(int)\n",
        "\n",
        "# Find Frequency of Items\n",
        "df.sum()\n",
        "# finding Product Frequency / Total Sales\n",
        "first = pd.DataFrame(df.sum() / df.shape[0], columns = [\"Support\"]).sort_values(\"Support\", ascending = False)\n",
        "first\n",
        "# Elimination by Support Value\n",
        "first[first.Support >= 0.13]\n",
        "#Second Iteration: Find support values for pair product combinations.\n",
        "second = list(itertools.combinations(first.index, 2))\n",
        "second = [list(i) for i in second]\n",
        "# Sample of combinations\n",
        "second[:10]\n",
        "# Finding support values\n",
        "value = []\n",
        "for i in range(0, len(second)):\n",
        "    temp = df.T.loc[second[i]].sum()\n",
        "    temp = len(temp[temp == df.T.loc[second[i]].shape[0]]) / df.shape[0]\n",
        "    value.append(temp)\n",
        "# Create a data frame\n",
        "secondIteration = pd.DataFrame(value, columns = [\"Support\"])\n",
        "secondIteration[\"index\"] = [tuple(i) for i in second]\n",
        "secondIteration['length'] = secondIteration['index'].apply(lambda x:len(x))\n",
        "secondIteration = secondIteration.set_index(\"index\").sort_values(\"Support\", ascending = False)\n",
        "# Elimination by Support Value\n",
        "secondIteration = secondIteration[secondIteration.Support > 0.1]\n",
        "secondIteration\n",
        "def ar_iterations(data, num_iter = 1, support_value = 0.1, iterationIndex = None):\n",
        "\n",
        "    # Next Iterations\n",
        "    def ar_calculation(iterationIndex = iterationIndex):\n",
        "        # Calculation of support value\n",
        "        value = []\n",
        "        for i in range(0, len(iterationIndex)):\n",
        "            result = data.T.loc[iterationIndex[i]].sum()\n",
        "            result = len(result[result == data.T.loc[iterationIndex[i]].shape[0]]) / data.shape[0]\n",
        "            value.append(result)\n",
        "        # Bind results\n",
        "        result = pd.DataFrame(value, columns = [\"Support\"])\n",
        "        result[\"index\"] = [tuple(i) for i in iterationIndex]\n",
        "        result['length'] = result['index'].apply(lambda x:len(x))\n",
        "        result = result.set_index(\"index\").sort_values(\"Support\", ascending = False)\n",
        "        # Elimination by Support Value\n",
        "        result = result[result.Support > support_value]\n",
        "        return result\n",
        "\n",
        "    # First Iteration\n",
        "    first = pd.DataFrame(df.T.sum(axis = 1) / df.shape[0], columns = [\"Support\"]).sort_values(\"Support\", ascending = False)\n",
        "    first = first[first.Support > support_value]\n",
        "    first[\"length\"] = 1\n",
        "\n",
        "    if num_iter == 1:\n",
        "        res = first.copy()\n",
        "\n",
        "    # Second Iteration\n",
        "    elif num_iter == 2:\n",
        "\n",
        "        second = list(itertools.combinations(first.index, 2))\n",
        "        second = [list(i) for i in second]\n",
        "        res = ar_calculation(second)\n",
        "\n",
        "    # All Iterations > 2\n",
        "    else:\n",
        "        nth = list(itertools.combinations(set(list(itertools.chain(*iterationIndex))), num_iter))\n",
        "        nth = [list(i) for i in nth]\n",
        "        res = ar_calculation(nth)\n",
        "\n",
        "    return res\n",
        "iteration1 = ar_iterations(df, num_iter=1, support_value=0.1)\n",
        "iteration1\n",
        "iteration2 = ar_iterations(df, num_iter=2, support_value=0.1)\n",
        "iteration2\n",
        "iteration3 = ar_iterations(df, num_iter=3, support_value=0.01,\n",
        "              iterationIndex=iteration2.index)\n",
        "iteration3\n",
        "iteration4 = ar_iterations(df, num_iter=4, support_value=0.01,\n",
        "              iterationIndex=iteration3.index)\n",
        "iteration4\n",
        "# Apriori\n",
        "freq_items = apriori(df, min_support = 0.1, use_colnames = True, verbose = 1)\n",
        "freq_items.sort_values(\"support\", ascending = False)\n",
        "freq_items.sort_values(\"support\", ascending = False).head(5)\n",
        "freq_items.sort_values(\"support\", ascending = False).tail(5)\n",
        "# Association Rules & Info\n",
        "df_ar = association_rules(freq_items, metric = \"confidence\", min_threshold = 0.5)\n",
        "df_ar\n",
        "df_ar[(df_ar.support > 0.15) & (df_ar.confidence > 0.5)].sort_values(\"confidence\", ascending = False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a8PxaIQYvNJ3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-14 03:07:48.215 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
            "2024-01-14 03:07:48.347 Serialization of dataframe to Arrow table was unsuccessful due to: (\"Could not convert frozenset({'BISCUIT'}) with type frozenset: did not recognize Python value type when inferring an Arrow data type\", 'Conversion failed for column itemsets with type object'). Applying automatic fixes for column types to make the dataframe Arrow-compatible.\n",
            "2024-01-14 03:07:48.400 Serialization of dataframe to Arrow table was unsuccessful due to: (\"Could not convert frozenset({'COCK'}) with type frozenset: did not recognize Python value type when inferring an Arrow data type\", 'Conversion failed for column antecedents with type object'). Applying automatic fixes for column types to make the dataframe Arrow-compatible.\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "import itertools\n",
        "\n",
        "# Load the grocery dataset\n",
        "data = \"\"\"JAM,MAGGI,BREAD,MILK\n",
        "MAGGI,TEA,BISCUIT\n",
        "BREAD,TEA,BOURNVITA\n",
        "MAGGI,TEA,CORNFLAKES\n",
        "MAGGI,BREAD,TEA,BISCUIT\n",
        "JAM,MAGGI,BREAD,TEA\n",
        "BREAD,MILK\n",
        "COFFEE,COCK,BISCUIT,CORNFLAKES\n",
        "COFFEE,COCK,BISCUIT,CORNFLAKES\n",
        "COFFEE,SUGER,BOURNVITA\n",
        "BREAD,COFFEE,COCK\n",
        "BREAD,SUGER,BISCUIT\n",
        "COFFEE,SUGER,CORNFLAKES\n",
        "BREAD,SUGER,BOURNVITA\n",
        "BREAD,COFFEE,SUGER\n",
        "BREAD,COFFEE,SUGER\n",
        "TEA,MILK,COFFEE,CORNFLAKES\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_csv(StringIO(data), header=None)\n",
        "\n",
        "# Convert all values to strings\n",
        "df = df.applymap(str)\n",
        "\n",
        "# Preprocess the data\n",
        "te = TransactionEncoder()\n",
        "te_data = te.fit(df.values).transform(df.values)\n",
        "df_encoded = pd.DataFrame(te_data, columns=te.columns_)\n",
        "\n",
        "# Apriori function\n",
        "def run_apriori(data, min_support=0.1, min_confidence=0.5):\n",
        "    frequent_items = apriori(data, min_support=min_support, use_colnames=True)\n",
        "    rules = association_rules(frequent_items, metric=\"confidence\", min_threshold=min_confidence)\n",
        "    return frequent_items, rules\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Apriori Association Rules App\")\n",
        "\n",
        "# User input for item selection\n",
        "selected_item = st.selectbox(\"Select an item to check for association rules\", df.columns)\n",
        "\n",
        "# Get frequent items and association rules\n",
        "frequent_items, rules = run_apriori(df_encoded)\n",
        "\n",
        "# Display frequent items\n",
        "st.subheader(\"Frequent Items\")\n",
        "st.write(frequent_items)\n",
        "\n",
        "# Display association rules\n",
        "st.subheader(\"Association Rules\")\n",
        "st.write(rules)\n",
        "\n",
        "# Display association rules for the selected item\n",
        "st.subheader(f\"Association Rules for {selected_item}\")\n",
        "filtered_rules = rules[rules[\"consequents\"].apply(lambda x: selected_item in x)]\n",
        "st.write(filtered_rules)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1135476472.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    streamlit run\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "streamlit run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
